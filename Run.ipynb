{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more detailed view of how the evaluation was made, you can go to work/Evaluation.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model initilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple model is effectively a dictionary of weights for each token. It requires a preprocessing step on the input data to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "pattern = r'<num>|<date>|<email>|<url>|\\w+|[^\\w\\s]'\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def classify(sequence, token_importance):\n",
    "    tokens = sequence.split()\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in token_importance:\n",
    "            score += token_importance[token]\n",
    "    return score\n",
    "\n",
    "def bin_classify(sequence, token_importance):\n",
    "    score = classify(sequence, token_importance)\n",
    "    return score > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "simple_model = pd.read_csv('simple_model_importance.csv')\n",
    "\n",
    "token_importance = dict(zip(simple_model['token'], simple_model['importance']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model evaluation (FakeNews + LIAR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by running FakeNews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "complete_data = pd.read_parquet('pre_processed_news.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(complete_data['cleaned_content'], complete_data['type'], test_size=0.2, random_state=42)\n",
    "y_train = y_train.apply(lambda x: True if x in [\"reliable\", \"political\"] else False)\n",
    "y_test = y_test.apply(lambda x: True if x in [\"reliable\", \"political\"] else False)\n",
    "\n",
    "# Let's split test into test and validation:\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# In this notebook, we're only goiing to use the _val sets for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying preproecssing on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the F1, accuracy, and matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "simple_validation_pred = X_val.apply(lambda x: bin_classify(' '.join(x), token_importance))\n",
    "\n",
    "# measure accuracy in F1 score:\n",
    "print(\"F1: \")\n",
    "print(f1_score(y_val, simple_validation_pred, average='weighted'))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy: \")\n",
    "print(accuracy_score(y_val, simple_validation_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, simple_validation_pred))\n",
    "\n",
    "# also confusion matrix:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_val, simple_validation_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can try this on the LIAR dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('LIAR/train.tsv', sep='\\t', header=None)\n",
    "bin_data = df.replace({1: {'barely-true': True, 'true': True, 'mostly-true': True, 'half-true': False, 'pants-fire': False, 'false': False}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict:\n",
    "bin_data[2] = bin_data[2].apply(preprocess)\n",
    "simple_model_pred = bin_data[2].apply(lambda x: bin_classify(' '.join(x), token_importance))\n",
    "\n",
    "# measure accuracy in F1 score:\n",
    "print(\"F1: \")\n",
    "print(f1_score(bin_data[1], simple_model_pred, average='weighted'))\n",
    "\n",
    "print(\"Accuracy: \")\n",
    "print(accuracy_score(bin_data[1], simple_model_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(bin_data[1], simple_model_pred))\n",
    "\n",
    "print(confusion_matrix(bin_data[1], simple_model_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex model (GRU) initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    complex_tokenizer = pickle.load(handle)\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "complex_model = keras.models.load_model('model3.h5')\n",
    "\n",
    "print(complex_model.summary())\n",
    "print(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex model evaluation(FakeNews + LIAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we start with FakeNews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "complete_data = pd.read_parquet('pre_processed_news.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(complete_data['cleaned_content'], complete_data['type'], test_size=0.2, random_state=42)\n",
    "y_train = y_train.apply(lambda x: True if x in [\"reliable\", \"political\"] else False)\n",
    "y_test = y_test.apply(lambda x: True if x in [\"reliable\", \"political\"] else False)\n",
    "\n",
    "# Let's split test into test and validation:\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# In this notebook, we're only goiing to use the _val sets for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_val)\n",
    "padded = pad_sequences(sequences, maxlen=800, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step actually runs the model and can take a while to run since its not set to run on GPU (since I don't know what machines you will run this on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions:\n",
    "predictions = complex_model.predict(padded)\n",
    "\n",
    "# Binarize the predictions:\n",
    "threshold = 0.5\n",
    "bin_predictions = np.where(predictions > threshold, 1, 0)\n",
    "bin_predictions = np.where(bin_predictions == 1, True, False)\n",
    "\n",
    "print(\"F1: \")\n",
    "print(f1_score(y_val, bin_predictions, average='weighted'))\n",
    "\n",
    "print(\"Accuracy: \")\n",
    "print(accuracy_score(y_val, bin_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_val, bin_predictions))\n",
    "labels = ['real', 'fake']\n",
    "print(metrics.classification_report(y_val, bin_predictions, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then LIAR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('LIAR/train.tsv', sep='\\t', header=None)\n",
    "bin_data = df.replace({1: {'barely-true': True, 'true': True, 'mostly-true': True, 'half-true': False, 'pants-fire': False, 'false': False}})\n",
    "\n",
    "print(bin_data[1].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the sequences:\n",
    "sequences = tokenizer.texts_to_sequences(bin_data[2])\n",
    "padded = pad_sequences(sequences, maxlen=50, padding='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make predictions:\n",
    "predictions = complex_model.predict(padded)\n",
    "\n",
    "threshold = 0.5\n",
    "bin_predictions = np.where(predictions > threshold, 1, 0)\n",
    "bin_predictions = np.where(bin_predictions == 1, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame({'text': bin_data[2], 'predictions': bin_predictions[:,0]})\n",
    "new_df.to_csv('complex_model_predictions.csv', index=False)\n",
    "#^ These predictions were exported to a csv file for further analysis, but strictly speaking, this is unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_preds = pd.read_csv('complex_model_predictions.csv')\n",
    "# classificaiton report:\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# using complex_prds:\n",
    "print(classification_report(bin_data[1], complex_preds['predictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f-score and accuracy:\n",
    "print(\"F1: \")\n",
    "print(f1_score(bin_data[1], complex_preds['predictions'], average='weighted'))\n",
    "\n",
    "print(\"Accuracy: \")\n",
    "print(accuracy_score(bin_data[1], complex_preds['predictions']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
