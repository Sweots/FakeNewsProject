{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing is a result of the work done in work/data_prep_1.ipynb, work/data_prep_2.ipynb, and work/data_prep_3.ipynb. It isn't included here for brevity since its very long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "complete_data = pd.read_parquet('p4_rich_tokens.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a simple classification model: Fake news or real news.\n",
    "# For each token, we calculate the average occurance rate in the fake news and in the real news.\n",
    "# We take the ratios of average occurance rates to calculate the importance of each token in the classification.\n",
    "# The simple model will be a weighted sum of the real-ness of each token and the fake-ness of each token.\n",
    "# The final classification will be the sign of the weighted sum.\n",
    "# so no training required, since we are using the average occurance rates:\n",
    "\n",
    "def vocabulary(rich_tokens):\n",
    "    vocab = set()\n",
    "    for tokens_string in rich_tokens:\n",
    "        # Split the string into tokens based on spaces\n",
    "        tokens = tokens_string.split(\" \")\n",
    "        # Update the set with these tokens, not characters\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "fake_data = complete_data[complete_data['type'].isin(['fake', 'conspiracy', 'bias', 'hate'])]\n",
    "real_data = complete_data[complete_data['type'].isin(['reliable', 'political'])]\n",
    "\n",
    "fake_rich_tokens = fake_data['rich_tokens']\n",
    "real_rich_tokens = real_data['rich_tokens']\n",
    "\n",
    "fake_vocab = vocabulary(fake_rich_tokens)\n",
    "real_vocab = vocabulary(real_rich_tokens)\n",
    "complete_vocab = fake_vocab.union(real_vocab)\n",
    "\n",
    "print('Number of tokens in the complete vocabulary:', len(complete_vocab))\n",
    "print('Number of tokens in the fake vocabulary:', len(fake_vocab))\n",
    "print('Number of tokens in the real vocabulary:', len(real_vocab))\n",
    "\n",
    "# Redo, but this time we split the data into 80, 10, 10 percent for training, validation, and testing, respectively.\n",
    "\n",
    "# Split the data into training, validation, and testing sets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(complete_data, test_size=0.2, random_state=42)\n",
    "validation_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_fake_data = train_data[train_data['type'].isin(['fake', 'conspiracy', 'bias', 'hate'])]\n",
    "train_real_data = train_data[train_data['type'].isin(['reliable', 'political'])]\n",
    "\n",
    "train_fake_rich_tokens = train_fake_data['rich_tokens']\n",
    "train_real_rich_tokens = train_real_data['rich_tokens']\n",
    "\n",
    "train_fake_vocab = vocabulary(train_fake_rich_tokens)\n",
    "train_real_vocab = vocabulary(train_real_rich_tokens)\n",
    "train_complete_vocab = train_fake_vocab.union(train_real_vocab)\n",
    "\n",
    "\n",
    "print('Number of tokens in the train complete vocabulary:', len(train_complete_vocab))\n",
    "print('Number of tokens in the train fake vocabulary:', len(train_fake_vocab))\n",
    "print('Number of tokens in the train real vocabulary:', len(train_real_vocab))\n",
    "\n",
    "# Again for validation and testing:\n",
    "validation_fake_data = validation_data[validation_data['type'].isin(['fake', 'conspiracy', 'bias', 'hate'])]\n",
    "validation_real_data = validation_data[validation_data['type'].isin(['reliable', 'political'])]\n",
    "\n",
    "validation_fake_rich_tokens = validation_fake_data['rich_tokens']\n",
    "validation_real_rich_tokens = validation_real_data['rich_tokens']\n",
    "\n",
    "validation_fake_vocab = vocabulary(validation_fake_rich_tokens)\n",
    "validation_real_vocab = vocabulary(validation_real_rich_tokens)\n",
    "validation_complete_vocab = validation_fake_vocab.union(validation_real_vocab)\n",
    "\n",
    "# and for testing:\n",
    "test_fake_data = test_data[test_data['type'].isin(['fake', 'conspiracy', 'bias', 'hate'])]\n",
    "test_real_data = test_data[test_data['type'].isin(['reliable', 'political'])]\n",
    "\n",
    "test_fake_rich_tokens = test_fake_data['rich_tokens']\n",
    "test_real_rich_tokens = test_real_data['rich_tokens']\n",
    "\n",
    "test_fake_vocab = vocabulary(test_fake_rich_tokens)\n",
    "test_real_vocab = vocabulary(test_real_rich_tokens)\n",
    "test_complete_vocab = test_fake_vocab.union(test_real_vocab)\n",
    "\n",
    "complete_vocab = train_complete_vocab.union(validation_complete_vocab).union(test_complete_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def vocabulary(rich_tokens):\n",
    "    vocab = Counter()\n",
    "    for sequence in rich_tokens:\n",
    "        tokens = sequence.split()\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "complete_vocab = vocabulary(complete_data['rich_tokens'])\n",
    "train_fake_vocab = vocabulary(train_fake_data['rich_tokens'])\n",
    "train_real_vocab = vocabulary(train_real_data['rich_tokens'])\n",
    "\n",
    "print('Number of tokens in the complete vocabulary:', len(complete_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frequent_words = {word: count for word, count in complete_vocab.items() if count > 1000}\n",
    "\n",
    "train_fake_occurances = {word: count for word, count in train_fake_vocab.items() if word in frequent_words}\n",
    "train_real_occurances = {word: count for word, count in train_real_vocab.items() if word in frequent_words}\n",
    "\n",
    "print('Top 10 fake tokens:')\n",
    "print(sorted(train_fake_occurances.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "print('Top 10 real tokens:')\n",
    "print(sorted(train_real_occurances.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "\n",
    "\n",
    "train_fake_rich_tokens = train_fake_data['rich_tokens']\n",
    "train_real_rich_tokens = train_real_data['rich_tokens']\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def size_of_token_sequence_list_v2(sequence_list):\n",
    "    # same but if its in frequent_words\n",
    "    count = Counter()\n",
    "    for sequence in sequence_list:\n",
    "        tokens = sequence.split()\n",
    "        count.update([word for word in tokens if word in frequent_words])\n",
    "    \n",
    "    return sum(count.values())\n",
    "\n",
    "\n",
    "# if is in frequent_words:\n",
    "train_fake_richer_token_count = size_of_token_sequence_list_v2(train_fake_rich_tokens)\n",
    "train_real_richer_token_count = size_of_token_sequence_list_v2(train_real_rich_tokens)\n",
    "\n",
    "\n",
    "# average:\n",
    "train_fake_token_rates = {word: train_fake_occurances[word] / train_fake_richer_token_count for word in train_fake_vocab if word in frequent_words}\n",
    "train_real_token_rates = {word: train_real_occurances[word] / train_real_richer_token_count for word in train_real_vocab if word in frequent_words}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the importance of each token in the classification:\n",
    "train_token_importance = {}\n",
    "for token in frequent_words:\n",
    "    if token in train_fake_token_rates and token in train_real_token_rates:\n",
    "        if train_real_token_rates[token] > train_fake_token_rates[token]:\n",
    "            train_token_importance[token] = (train_real_token_rates[token] / train_fake_token_rates[token] -1)\n",
    "        else:\n",
    "            train_token_importance[token] = -((train_fake_token_rates[token] / train_real_token_rates[token]) - 1)\n",
    "\n",
    "    elif token in train_fake_token_rates:\n",
    "        train_token_importance[token] = 0\n",
    "    else:\n",
    "        train_token_importance[token] = 0 \n",
    "\n",
    "\n",
    "# Sort the tokens by importance:\n",
    "train_sorted_tokens = sorted(train_token_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the 10 most important tokens:\n",
    "print('The 50 most positive tokens:')\n",
    "print(train_sorted_tokens[:50])\n",
    "print('The 50 most negative tokens:')\n",
    "print(train_sorted_tokens[-50:])\n",
    "\n",
    "# most neutral tokens (lowest absolute value) that aren't exactly 0\n",
    "print('The 50 most neutral tokens:')\n",
    "print(sorted([x for x in train_token_importance.items() if x[1] != 0], key=lambda x: abs(x[1]))[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sequence, token_importance):\n",
    "    tokens = sequence.split()\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in token_importance:\n",
    "            score += token_importance[token]\n",
    "    return score\n",
    "\n",
    "def bin_classify(sequence, token_importance):\n",
    "    score = classify(sequence, token_importance)\n",
    "    return score > 0\n",
    "\n",
    "Y = train_data['type']\n",
    "Y_pred = train_data['rich_tokens'].apply(lambda x: bin_classify(x, train_token_importance))\n",
    "\n",
    "print('y_pred:', Y_pred)\n",
    "Y_bin = Y.isin(['reliable', 'political'])\n",
    "Y_pred_bin = Y_pred\n",
    "\n",
    "# we remve all rows with NaN values or values that aren't in the set {True, False} from both Y and Y_pred:\n",
    "# i.e. if Y has a NaN value at index i, remove index i from Y and Y_pred.\n",
    "mask = Y_bin.isin([True, False])\n",
    "Y_bin = Y_bin[mask]\n",
    "Y_pred_bin = Y_pred_bin[mask]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(Y_bin, Y_pred_bin)\n",
    "print('Training accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model. So the importance of each token, in csv:\n",
    "import csv\n",
    "\n",
    "\n",
    "with open('simple_model_importance.csv', 'w', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['token', 'importance'])\n",
    "    for token, importance in train_token_importance.items():\n",
    "        writer.writerow([token, importance])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Token importance:', train_token_importance)\n",
    "\n",
    "# Sum of positive importance:\n",
    "positive_importance = sum(importance for importance in train_token_importance.values() if importance > 0)\n",
    "# Sum of negative importance:\n",
    "negative_importance = sum(importance for importance in train_token_importance.values() if importance < 0)\n",
    "\n",
    "print('Sum of positive importance:', positive_importance)\n",
    "print('Sum of negative importance:', negative_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(the extra scraped data experiment was removed from this notebook but is present in work/part2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Complex model (preparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "complete_data = pd.read_parquet('p4_rich_tokens.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows with duplicate \"rich_tokens\" values:\n",
    "print('Removing duplicates...')\n",
    "complete_data = complete_data.drop_duplicates(subset='rich_tokens') \n",
    "# save as parquet again:\n",
    "complete_data.to_parquet('pre_processed_news.parquet')\n",
    "complete_data = pd.read_parquet('pre_processed_news.parquet')\n",
    "complete_data = complete_data.dropna(subset=['type'])\n",
    "complete_data[['type', 'cleaned_content']].to_parquet('pre_processed_news.parquet')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Complex model 1 (TF-IDF, Logreg, random forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(complete_data['cleaned_content'], complete_data['type'], test_size=0.2, random_state=42)\n",
    "y_train = y_train.apply(lambda x: True if x in [\"reliable\", \"political\"] else False)\n",
    "y_test = y_test.apply(lambda x: True if x in [\"reliable\", \"political\"] else False)\n",
    "\n",
    "# Let's split test into test and validation:\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# print:\n",
    "print('X_train: ', X_train.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('y_test: ', y_test.shape)\n",
    "print('X_val: ', X_val.shape)\n",
    "print('y_val: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=0.005)\n",
    "\n",
    "print('Fitting the vectorizer...')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "print('Done. matrix: ', X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "print('Fitting the model...')\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "print('TF-IDF Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log reg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now use logistic regression:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "print('Fitting the model...')\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict:\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "print('Logreg accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=15, max_features='sqrt', n_jobs=-1)\n",
    "print('Fitting the model...')\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature importances: ', clf.feature_importances_)\n",
    "print('Number of estimators: ', clf.n_estimators)\n",
    "print('Max depth: ', clf.max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "\n",
    "print('Random forest accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators=300, random_state=42, max_depth=15, max_features='sqrt', n_jobs=-1)\n",
    "print('Fitting the model...')\n",
    "clf2.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf2.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "\n",
    "print('Random forest attempt 2: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Complex model (GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The training of this segment was done in a kaggle notebook, to make use of the free GPUs they have. File references have been renamed here so that they can be accessed normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = pd.read_parquet('pre_processed_news.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(complete_data['cleaned_content'], complete_data['type'], test_size=0.2, random_state=42)\n",
    "y_train = y_train.apply(lambda x: True if x in [\"reliable\", \"political\"] else False)\n",
    "y_test = y_test.apply(lambda x: True if x in [\"reliable\", \"political\"] else False)\n",
    "\n",
    "# Let's split test into test and validation:\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print('X_train: ', X_train.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('y_test: ', y_test.shape)\n",
    "print('X_val: ', X_val.shape)\n",
    "print('y_val: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=25000) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "print(\"Fit\")\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('X_train_seq.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_train_seq, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('X_test_seq.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_test_seq, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/kaggle/input/fakenews-tokens/tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('/kaggle/input/fakenews-tokens/X_train_seq.pickle', 'rb') as handle:\n",
    "    X_train_seq = pickle.load(handle)\n",
    "\n",
    "# Loading X_test_seq\n",
    "with open('/kaggle/input/fakenews-tokens/X_test_seq.pickle', 'rb') as handle:\n",
    "    X_test_seq = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_seq, maxlen=800, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=800, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(X_train_pad)  # Total number of samples in your training data\n",
    "batch_size = 512  # Assuming this is the batch size you've chosen\n",
    "steps_per_epoch = total_samples // batch_size\n",
    "steps_per_epoch, total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tpu_strategy.scope():\n",
    "from keras.layers import Embedding, GRU, Dense, Dropout\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=25000, output_dim=52, input_shape=(800,)),  # Adjust these parameters as needed\n",
    "        GRU(units=324, return_sequences=True), \n",
    "        Dropout(0.2),\n",
    "        GRU(units=200, return_sequences=True), \n",
    "        Dropout(0.2),\n",
    "        GRU(units=64, return_sequences=False), \n",
    "        Dropout(0.2),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'], steps_per_execution=32)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np   \n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "train_gen = DataGenerator(X_train_pad, y_train, batch_size)\n",
    "test_gen = DataGenerator(X_test_pad, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit(X_train_pad, y_train, epochs=1,batch_size=BATCH_SIZE, validation_split=0.2)\n",
    "history = model.fit(train_gen, epochs=8, validation_data=test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model3.h5')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
